{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XBITN0M_LKds"
      },
      "source": [
        "# Face recognition and verification using the VGG2 dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdoDIKWOMF59"
      },
      "source": [
        "# Preliminaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip3 install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jwLEd0gdPbSc",
        "outputId": "b3286d1d-bbbf-4abd-ccfe-8b06f3348cd1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchsummary import summary\n",
        "import torchvision \n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import glob\n",
        "import wandb\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scOnMklwWBY6"
      },
      "source": [
        "# Unzip the data files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oFjaJTaRjT7",
        "outputId": "390b6bf9-b80f-43eb-ad09-dbaa81dfb729"
      },
      "outputs": [],
      "source": [
        "!unzip 'data/vgg_classification.zip'\n",
        "!unzip 'data/vgg_verification.zip'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O68hT27SXClj"
      },
      "source": [
        "# Configs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S7qpMxG0XCJz"
      },
      "outputs": [],
      "source": [
        "# initializing some hyperparameters in a global dict so we can refer to these downstream\n",
        "config = {\n",
        "    'architecture': 'convnext-t',\n",
        "    'optimizer' : 'SGD',\n",
        "    'lr': 1e-2,\n",
        "    'momentum': 0.9,\n",
        "    'loss' : 'cross entropy',\n",
        "    'scheduler': 'reduce on plateau',\n",
        "    'augmentations': 'Rand Augment',\n",
        "    'weight_decay': 1e-4,\n",
        "    'label_smoothing' : 0.1,\n",
        "    'stochastic_depth': 0.1,\n",
        "    'regularization': '',\n",
        "    'batch_size': 128,\n",
        "    'epochs': 50,\n",
        "}\n",
        "SAVE_PATH = '' # file path to save checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSeiKHYrM-6b"
      },
      "source": [
        "# Classification Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tmRX5omaNDEZ",
        "outputId": "a54bd7b8-64b3-45eb-921e-618da8cf0e23"
      },
      "outputs": [],
      "source": [
        "DATA_DIR = '/data/'\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, \"classification/train\")\n",
        "VAL_DIR = os.path.join(DATA_DIR, \"classification/dev\")\n",
        "TEST_DIR = os.path.join(DATA_DIR, \"classification/test\")\n",
        "\n",
        "# Transforms using torchvision - Refer https://pytorch.org/vision/stable/transforms.html\n",
        "# We can chain multiple transforms using 'Compose'\n",
        "\n",
        "train_transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.RandomHorizontalFlip(p=0.5), # laterally flipped faces\n",
        "    torchvision.transforms.RandomResizedCrop(size=224, scale=(0.25, 1)),\n",
        "    torchvision.transforms.ColorJitter(brightness=0.2, hue=0.0, contrast=0.2, saturation=0.2),\n",
        "    torchvision.transforms.RandomPerspective(distortion_scale=0.5, p=0.3), # faces from different perspectives\n",
        "    torchvision.transforms.RandAugment(), # effective in the convnext paper\n",
        "    torchvision.transforms.RandomGrayscale(p=0.1), # based on error analysis\n",
        "    torchvision.transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 5)), # based on error analysis\n",
        "    torchvision.transforms.ToTensor(),\n",
        "    torchvision.transforms.RandomErasing(p=0.5) # based on error analysis\n",
        "])\n",
        "\n",
        "\n",
        "# We dont perform augmentations on the val and test set\n",
        "val_transforms = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n",
        "\n",
        "\n",
        "train_dataset = torchvision.datasets.ImageFolder(TRAIN_DIR, transform = train_transforms)\n",
        "val_dataset = torchvision.datasets.ImageFolder(VAL_DIR, transform = val_transforms)\n",
        "\n",
        "\n",
        "# Create data loaders\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size = config['batch_size'],\n",
        "                                           shuffle = True,num_workers = 4, pin_memory = True)\n",
        "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size = config['batch_size'],\n",
        "                                         shuffle = False, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SqSR063BGE2e"
      },
      "outputs": [],
      "source": [
        "class ClassificationTestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, data_dir, transforms):\n",
        "        self.data_dir   = data_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "        # This generates a sorted list of full paths to each image in the test directory\n",
        "        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.transforms(Image.open(self.img_paths[idx]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVLB41KtGC2o"
      },
      "outputs": [],
      "source": [
        "test_dataset = ClassificationTestDataset(TEST_DIR, transforms = val_transforms) \n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = config['batch_size'], shuffle = False,\n",
        "                         drop_last = False, num_workers = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4t8eU9gY0Jy",
        "outputId": "2954fdd1-d56d-4761-c5f4-d7b5947ed28f"
      },
      "outputs": [],
      "source": [
        "print(\"Number of classes: \", len(train_dataset.classes))\n",
        "print(\"No. of train images: \", train_dataset.__len__())\n",
        "print(\"Shape of image: \", train_dataset[0][0].shape)\n",
        "print(\"Batch size: \", config['batch_size'])\n",
        "print(\"Train batches: \", train_loader.__len__())\n",
        "print(\"Val batches: \", val_loader.__len__())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIqmojPaWD0H"
      },
      "source": [
        "# Model Declaration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nS09PoF7cVU"
      },
      "source": [
        "This section contains code for a modified [convnext](https://arxiv.org/abs/2201.03545) model, we create a custom [stochastic depth](https://arxiv.org/abs/1603.09382) module and use batchmorm instead of layernorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ny-mh_ocWIJR"
      },
      "outputs": [],
      "source": [
        "class ConvNextBlock(torch.nn.Module):\n",
        "  def __init__(self, channels, stochastic_depth_p):\n",
        "      super(ConvNextBlock, self).__init__()\n",
        "      self.stochastic_depth_p = stochastic_depth_p\n",
        "      self.gelu = torch.nn.GELU()\n",
        "      self.block_pass = torch.nn.Sequential(\n",
        "                                      # Depth-wise convolution\n",
        "                                      torch.nn.Conv2d(channels[0], channels[0], kernel_size=7, stride=1, padding=3, groups=channels[0]),\n",
        "                                      torch.nn.BatchNorm2d(channels[0]),\n",
        "\n",
        "                                      # Point_wise convolution\n",
        "                                      torch.nn.Conv2d(channels[0], channels[1], kernel_size=1, stride=1, padding=0),\n",
        "                                      torch.nn.GELU(),\n",
        "\n",
        "                                      # Point_wise convolution\n",
        "                                      torch.nn.Conv2d(channels[1], channels[2], kernel_size=1, stride=1, padding=0)\n",
        "      )\n",
        "\n",
        "      self.stochastic_drop = torchvision.ops.StochasticDepth(stochastic_depth_p, mode='batch')\n",
        "\n",
        "  def forward(self, x):\n",
        "      residual = x\n",
        "      x = self.block_pass(x)\n",
        "      x = self.stochastic_drop(x)\n",
        "      return x + residual\n",
        "\n",
        "class DownSamplingBlock(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, ds_factor):\n",
        "      super(DownSamplingBlock, self).__init__()\n",
        "      self.ds = torch.nn.Sequential(\n",
        "                                      # Normalization before downsampling as described in 2.6\n",
        "                                      torch.nn.BatchNorm2d(in_channels),\n",
        "                                      torch.nn.Conv2d(in_channels, out_channels, kernel_size=2, stride=ds_factor)\n",
        "      )\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.ds(x)\n",
        "      return x\n",
        "\n",
        "class StemBlock(torch.nn.Module):\n",
        "  def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
        "    super(StemBlock, self).__init__()\n",
        "    self.block_pass = torch.nn.Sequential(\n",
        "                                    torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=0),\n",
        "                                    torch.nn.BatchNorm2d(out_channels)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    x = self.block_pass(x)\n",
        "    return x\n",
        "\n",
        "class ConvNextT(torch.nn.Module):\n",
        "  def __init__(self, ConvNextBlock, DownSamplingBlock, StemBlock, stochastic_depth_p=0.0, num_classes=7000):\n",
        "    super(ConvNextT, self).__init__()\n",
        "    # In = b x 3 x 224 x 224\n",
        "    self.stem = StemBlock(in_channels=3, out_channels=96, kernel_size=4, stride=4)\n",
        "    # Out = b x 96 x 56 x 56\n",
        "\n",
        "    # In = b x 96 x 56 x 56\n",
        "    self.res2 = self._make_layer(channels=[96, 384, 96], num_blocks=3, stochastic_depth_p=stochastic_depth_p)\n",
        "    self.ds2 = DownSamplingBlock(in_channels=96, out_channels=192, ds_factor=2)\n",
        "    # Out = b x 192 x 28 x 28\n",
        "\n",
        "    # In = b x 192 x 28 x 28\n",
        "    self.res3 = self._make_layer(channels=[192, 768, 192], num_blocks=3, stochastic_depth_p=stochastic_depth_p)\n",
        "    self.ds3 = DownSamplingBlock(in_channels=192, out_channels=384, ds_factor=2)\n",
        "    # Out = b x 384 x 14 x 14\n",
        "\n",
        "    # In = b x 384 x 14 x 14\n",
        "    self.res4 = self._make_layer(channels=[384, 1536, 384], num_blocks=9, stochastic_depth_p=stochastic_depth_p)\n",
        "    self.ds4 = DownSamplingBlock(in_channels=384, out_channels=768, ds_factor=2)\n",
        "    # Out = b x 768 x 7 x 7\n",
        "\n",
        "    # In = b x 768 x 7 x 7\n",
        "    self.res5 = self._make_layer(channels=[768, 3072, 768], num_blocks=3, stochastic_depth_p=stochastic_depth_p)\n",
        "    # Out = b x 768 x 7 x 7\n",
        "\n",
        "    # In = b x 768 x 7 x 7\n",
        "    self.avg = torch.nn.AdaptiveAvgPool2d(1)\n",
        "    # In = b x 768 x 1 x 1\n",
        "\n",
        "    self.fc = torch.nn.Sequential(\n",
        "                                    torch.nn.BatchNorm1d(768),\n",
        "                                    torch.nn.Linear(768, num_classes)\n",
        "    )\n",
        "\n",
        "\n",
        "  def _make_layer(self, channels, num_blocks, stochastic_depth_p):\n",
        "    layer = []\n",
        "\n",
        "    for _ in range(num_blocks):\n",
        "      block = ConvNextBlock(channels, stochastic_depth_p)\n",
        "      layer.append(block)\n",
        "\n",
        "    return torch.nn.Sequential(*layer)\n",
        "\n",
        "  def forward(self, x, return_feats=False):\n",
        "    # stem layer\n",
        "    x = self.stem(x)\n",
        "\n",
        "    # res2 layer\n",
        "    x = self.res2(x)\n",
        "    x = self.ds2(x)\n",
        "\n",
        "    # res3 layer\n",
        "    x = self.res3(x)\n",
        "    x = self.ds3(x)\n",
        "\n",
        "    # res4 layer\n",
        "    x = self.res4(x)\n",
        "    x = self.ds4(x)\n",
        "\n",
        "    # res5 layer\n",
        "    x = self.res5(x)\n",
        "\n",
        "    # average pooling\n",
        "    x = self.avg(x)\n",
        "\n",
        "    # flatten\n",
        "    x = torch.flatten(x, start_dim=1)\n",
        "    if return_feats:\n",
        "      return x\n",
        "\n",
        "    # classifier layer\n",
        "    x = self.fc(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "model = ConvNextT(ConvNextBlock, DownSamplingBlock, StemBlock, stochastic_depth_p=config[\"stochastic_depth\"]).to(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUR2yQh88GrZ"
      },
      "source": [
        "This is the network I use to implement [Arc-face loss](https://arxiv.org/abs/1801.07698), it takes the convolutional network in and produces an output which corresponds to the ArcFace instructions, it is optimized with cross entropy loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UNn4mwfwiLHN"
      },
      "outputs": [],
      "source": [
        "\n",
        "class ArcFaceModel(torch.nn.Module):\n",
        "  def __init__(self, margin, scaler, classifier, embedding_size=768, num_classes=7000):\n",
        "    super(ArcFaceModel, self).__init__()\n",
        "    self.embedding_size = embedding_size\n",
        "    self.num_classes = num_classes\n",
        "    self.eps = 1e-7\n",
        "\n",
        "    self.margin = margin\n",
        "    self.scaler = scaler\n",
        "    self.classifier = classifier\n",
        "\n",
        "    self.AFL_linear = torch.nn.Linear(embedding_size, num_classes, bias=False)\n",
        "    self.AFL_linear.weight = self.classifier.fc[1].weight\n",
        "\n",
        "    self.normalizer = torch.nn.functional.normalize\n",
        "\n",
        "    self.arcCos = torch.acos\n",
        "\n",
        "    self.one_hot = torch.nn.functional.one_hot\n",
        "    self.cos = torch.cos\n",
        "\n",
        "\n",
        "\n",
        "  def forward(self, x, label):\n",
        "    # Get face embedding and normalize it\n",
        "    embedding = self.classifier(x, return_feats=True)\n",
        "    embedding = self.normalizer(embedding, dim=1)\n",
        "\n",
        "    # normalize linear layer weights\n",
        "    with torch.no_grad():\n",
        "      self.AFL_linear.weight = torch.nn.Parameter(self.normalizer(self.AFL_linear.weight, dim=1))\n",
        "\n",
        "    # take dot product to get cos theta\n",
        "    cosine = self.AFL_linear(embedding)\n",
        "    cosine = torch.clamp(cosine, min=-1.0+self.eps, max=1.0-self.eps)\n",
        "\n",
        "    # get theta by performing arccos(cos(theta))\n",
        "    theta = self.arcCos(cosine)\n",
        "\n",
        "    # To add 'm' to the corrrect class we need to generate a one hot vector representing the correct class\n",
        "    one_hot_labels = self.one_hot(label, self.num_classes)\n",
        "    margin = one_hot_labels * self.margin # margin will be zero everywhere except ground truth values\n",
        "    theta_m = theta + margin\n",
        "\n",
        "    # we take the cosine value and clamp it, then pass the output to crossEntropyLoss\n",
        "    logits = self.cos(theta_m) * self.scaler\n",
        "    return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZCn0qHuZRKj"
      },
      "source": [
        "# Setup everything for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UowI9OcUYPjP"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss(label_smoothing=config[\"label_smoothing\"]) \n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=config[\"lr\"], momentum=config[\"momentum\"], weight_decay=config[\"weight_decay\"])\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience = 4, mode='max', min_lr=0.0005) \n",
        "scaler = torch.cuda.amp.GradScaler() # Mixed precision training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WSdxQAOn7ffy",
        "outputId": "540057fc-f9c6-4727-f8de-d3632a7431e0"
      },
      "outputs": [],
      "source": [
        "model = ConvNextT(ConvNextBlock, DownSamplingBlock, StemBlock, stochastic_depth_p=0.1)\n",
        "model = ArcFaceModel(margin=0.5, scaler=64, classifier=model)\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dzM11HtcboYv"
      },
      "source": [
        "# Helper functions for training and validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgSw6iJJavBZ"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion, return_feats=False):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Progress Bar\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n",
        "\n",
        "    num_correct = 0\n",
        "    total_loss = 0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        optimizer.zero_grad() # Zero gradients\n",
        "\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        with torch.cuda.amp.autocast(): # This implements mixed precision. \n",
        "            outputs = model(images, return_feats)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "    \n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        \n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct,\n",
        "            lr=\"{:.04f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer) \n",
        "        scaler.update()\n",
        "\n",
        "        batch_bar.update() \n",
        "\n",
        "    batch_bar.close() \n",
        "\n",
        "    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5V2UdnpdEoK"
      },
      "outputs": [],
      "source": [
        "def validate(model, dataloader, criterion):\n",
        "\n",
        "    model.eval()\n",
        "    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n",
        "\n",
        "    num_correct = 0.0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for i, (images, labels) in enumerate(dataloader):\n",
        "\n",
        "        # Move images to device\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Get model outputs\n",
        "        with torch.inference_mode():\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n",
        "        total_loss += float(loss.item())\n",
        "\n",
        "        batch_bar.set_postfix(\n",
        "            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n",
        "            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n",
        "            num_correct=num_correct)\n",
        "\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n",
        "    total_loss = float(total_loss / len(dataloader))\n",
        "    return acc, total_loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mBgKGkXLrdJ"
      },
      "source": [
        "# Wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ix62_BkaLr_D"
      },
      "outputs": [],
      "source": [
        "wandb.login(key=\"\") "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VG0vmsmbRYEi"
      },
      "outputs": [],
      "source": [
        "\n",
        "run = wandb.init(\n",
        "    name = \"ConvNext-T V5 fine-tuning after AFL\",\n",
        "    reinit = True, \n",
        "    # run_id = \n",
        "    # resume = \n",
        "    project = \"ablations\", \n",
        "    config = config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SQkRw1FvLqYe"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "id": "EqWO8Edb0BK2",
        "outputId": "8fd42f23-82cf-4efd-fdaf-aa360364990d"
      },
      "outputs": [],
      "source": [
        "best_valacc = 0.0\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
        "\n",
        "    train_acc, train_loss = train(model, train_loader, optimizer, criterion)\n",
        "\n",
        "    val_acc, val_loss = validate(model, val_loader, criterion)\n",
        "    scheduler.step(val_acc)\n",
        "    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n",
        "        epoch + 1,\n",
        "        config['epochs'],\n",
        "        train_acc,\n",
        "        train_loss,\n",
        "        curr_lr))\n",
        "\n",
        "    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n",
        "\n",
        "    wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc,\n",
        "               'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n",
        "\n",
        "\n",
        "    if val_acc >= best_valacc:\n",
        "      print(\"Saving model\")\n",
        "      torch.save({'model_state_dict':model.state_dict(),\n",
        "                  'optimizer_state_dict':optimizer.state_dict(),\n",
        "                  'scheduler_state_dict':scheduler.state_dict(),\n",
        "                  'val_acc': val_acc,\n",
        "                  'epoch': epoch}, SAVE_PATH)\n",
        "      best_valacc = val_acc\n",
        "      wandb.save(SAVE_PATH)\n",
        "\n",
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "known_regex = \"/content/data/verification/known/*/*\"\n",
        "known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))]\n",
        "# This obtains the list of known identities from the known folder\n",
        "\n",
        "unknown_regex_dev = \"/content/data/verification/unknown_dev/*\" \n",
        "unknown_regex = \"/content/data/verification/unknown_test/*\"\n",
        "\n",
        "# We load the images from known and unknown folders\n",
        "unknown_images_dev = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_regex_dev)))]\n",
        "unknown_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_regex)))]\n",
        "known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]\n",
        "\n",
        "transforms = torchvision.transforms.Compose([\n",
        "    torchvision.transforms.ToTensor()])\n",
        "\n",
        "unknown_images_dev = torch.stack([transforms(x) for x in unknown_images_dev])\n",
        "unknown_images = torch.stack([transforms(x) for x in unknown_images])\n",
        "known_images  = torch.stack([transforms(y) for y in known_images ])\n",
        "\n",
        "similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def eval_verification(unknown_images, known_images, model, similarity, batch_size= config['batch_size'], mode='val', return_feats=True):\n",
        "\n",
        "    unknown_feats, known_feats = [], []\n",
        "\n",
        "    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "    model.eval()\n",
        "\n",
        "    # We load the images as batches for memory optimization and avoiding CUDA OOM errors\n",
        "    for i in range(0, unknown_images.shape[0], batch_size):\n",
        "        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size\n",
        "\n",
        "        with torch.no_grad():\n",
        "            unknown_feat = model(unknown_batch.float().to(device), return_feats=return_feats) #Get features from model\n",
        "        unknown_feats.append(unknown_feat)\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n",
        "\n",
        "    for i in range(0, known_images.shape[0], batch_size):\n",
        "        known_batch = known_images[i:i+batch_size]\n",
        "        with torch.no_grad():\n",
        "              known_feat = model(known_batch.float().to(device), return_feats=return_feats)\n",
        "\n",
        "        known_feats.append(known_feat)\n",
        "        batch_bar.update()\n",
        "\n",
        "    batch_bar.close()\n",
        "\n",
        "    # Concatenate all the batches\n",
        "    unknown_feats = torch.cat(unknown_feats, dim=0)\n",
        "    known_feats = torch.cat(known_feats, dim=0)\n",
        "\n",
        "    similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])\n",
        "\n",
        "    predictions = similarity_values.argmax(0).cpu().numpy() \n",
        "\n",
        "    # Map argmax indices to identity strings\n",
        "    pred_id_strings = [known_paths[i] for i in predictions]\n",
        "\n",
        "    if mode == 'val':\n",
        "      true_ids = pd.read_csv('/content/data/verification/dev_identities.csv')['label'].tolist()\n",
        "      accuracy = accuracy_score(pred_id_strings, true_ids)\n",
        "      print(\"Verification Accuracy = {}\".format(accuracy))\n",
        "\n",
        "    return pred_id_strings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pred_id_strings = eval_verification(unknown_images, known_images, model, similarity_metric, config['batch_size'], mode='test', return_feats=False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
